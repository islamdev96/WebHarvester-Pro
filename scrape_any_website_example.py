"""Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø£ÙŠ Ù…ÙˆÙ‚Ø¹"""

from universal_scraper import UniversalScraper
import json

def scrape_quotes_website():
    """Ù…Ø«Ø§Ù„: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª Ù…Ù† Ù…ÙˆÙ‚Ø¹ quotes.toscrape.com"""
    
    config = {
        'base_url': 'http://quotes.toscrape.com',
        'selectors': {
            'container': '.quote',           # ÙƒÙ„ Ø§Ù‚ØªØ¨Ø§Ø³
            'text': '.text',                 # Ù†Øµ Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³
            'author': '.author',             # Ø§Ù„Ù…Ø¤Ù„Ù
            'tags': '.tags .tag'             # Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª
        },
        'pagination': {
            'type': 'page_number',
            'pattern': '/page/{}'
        }
    }
    
    scraper = UniversalScraper(config)
    quotes = scraper.scrape_all_pages(max_pages=3)
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    with open('output/quotes_data.json', 'w', encoding='utf-8') as f:
        json.dump({
            'total_quotes': len(quotes),
            'quotes': quotes
        }, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(quotes)} Ø§Ù‚ØªØ¨Ø§Ø³")
    return quotes

def scrape_news_website():
    """Ù…Ø«Ø§Ù„: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± (ØªØ­ØªØ§Ø¬ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ§Ù„Ù…Ø­Ø¯Ø¯Ø§Øª)"""
    
    config = {
        'base_url': 'https://example-news.com',
        'selectors': {
            'container': '.news-item',
            'title': 'h2',
            'summary': '.summary',
            'date': '.date',
            'link': 'a'
        },
        'pagination': {
            'type': 'page_number',
            'pattern': '?page={}'
        }
    }
    
    scraper = UniversalScraper(config)
    news = scraper.scrape_all_pages(max_pages=2)
    
    print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(news)} Ø®Ø¨Ø±")
    return news

def scrape_products_website():
    """Ù…Ø«Ø§Ù„: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ù…Ù† Ù…ÙˆÙ‚Ø¹ ØªØ¬Ø§Ø±ÙŠ"""
    
    config = {
        'base_url': 'https://example-shop.com/products',
        'selectors': {
            'container': '.product-card',
            'title': '.product-name',
            'price': '.price',
            'image': 'img',
            'rating': '.rating',
            'link': 'a'
        },
        'pagination': {
            'type': 'page_number',
            'pattern': '?page={}'
        }
    }
    
    scraper = UniversalScraper(config)
    products = scraper.scrape_all_pages(max_pages=5)
    
    print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(products)} Ù…Ù†ØªØ¬")
    return products

if __name__ == "__main__":
    print("ğŸŒ Ø£Ù…Ø«Ù„Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…ÙˆØ§Ù‚Ø¹ Ù…Ø®ØªÙ„ÙØ©")
    print("=" * 50)
    
    # Ø¬Ø±Ø¨ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª (Ù…ÙˆÙ‚Ø¹ Ø­Ù‚ÙŠÙ‚ÙŠ Ù„Ù„ØªØ¬Ø±Ø¨Ø©)
    try:
        quotes = scrape_quotes_website()
        print("âœ… Ù†Ø¬Ø­ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª")
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª: {e}")
    
    print("\nğŸ’¡ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ù…ÙˆØ§Ù‚Ø¹ Ø£Ø®Ø±Ù‰:")
    print("1. ØºÙŠØ± base_url Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨")
    print("2. Ø§ÙØ­Øµ HTML Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙˆØ§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©")
    print("3. Ø¹Ø¯Ù„ selectors ÙÙŠ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    print("4. Ø´ØºÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬!")