"""Universal Web Scraper - ÙŠØ¹Ù…Ù„ Ù…Ø¹ Ø£ÙŠ Ù…ÙˆÙ‚Ø¹"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime

class UniversalScraper:
    """Ù…Ø³ØªØ®Ø±Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø§Ù… ÙŠØ¹Ù…Ù„ Ù…Ø¹ Ø£ÙŠ Ù…ÙˆÙ‚Ø¹"""
    
    def __init__(self, config):
        """
        config = {
            'base_url': 'https://example.com',
            'selectors': {
                'container': '.item',           # Ø­Ø§ÙˆÙŠ ÙƒÙ„ Ø¹Ù†ØµØ±
                'title': '.title',             # Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                'description': '.description', # Ø§Ù„ÙˆØµÙ
                'link': 'a',                   # Ø§Ù„Ø±Ø§Ø¨Ø·
                'image': 'img',                # Ø§Ù„ØµÙˆØ±Ø©
                'price': '.price',             # Ø§Ù„Ø³Ø¹Ø± (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
                'date': '.date'                # Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            },
            'pagination': {
                'type': 'page_number',         # Ø£Ùˆ 'next_button'
                'pattern': '?page={}'          # Ø£Ùˆ selector Ù„Ù„Ø²Ø± Ø§Ù„ØªØ§Ù„ÙŠ
            }
        }
        """
        self.config = config
        self.base_url = config['base_url']
        self.selectors = config['selectors']
        self.pagination = config.get('pagination', {})
        
    def scrape_page(self, url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø©"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ø§ÙˆÙŠØ§Øª Ø§Ù„Ø¹Ù†Ø§ØµØ±
            containers = soup.select(self.selectors['container'])
            
            items = []
            for i, container in enumerate(containers):
                item = self.extract_item_data(container, i+1)
                if item:
                    items.append(item)
            
            return items
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© {url}: {e}")
            return []
    
    def extract_item_data(self, container, index):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù†ØµØ± ÙˆØ§Ø­Ø¯"""
        item = {
            'id': f"item_{index}",
            'extracted_at': datetime.now().isoformat()
        }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø­Ù‚Ù„ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø§Øª
        for field_name, selector in self.selectors.items():
            if field_name == 'container':
                continue
                
            try:
                element = container.select_one(selector)
                if element:
                    if field_name == 'link':
                        item[field_name] = element.get('href', '')
                    elif field_name == 'image':
                        item[field_name] = element.get('src', '')
                    else:
                        item[field_name] = element.get_text().strip()
            except Exception as e:
                print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ {field_name}: {e}")
                item[field_name] = ""
        
        return item
    
    def scrape_all_pages(self, max_pages=None):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø§Ù„ØµÙØ­Ø§Øª"""
        all_items = []
        page_num = 1
        
        while True:
            if max_pages and page_num > max_pages:
                break
                
            # Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø©
            if page_num == 1:
                url = self.base_url
            else:
                if self.pagination.get('type') == 'page_number':
                    pattern = self.pagination.get('pattern', '?page={}')
                    url = self.base_url + pattern.format(page_num)
                else:
                    break  # Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø£Ù†ÙˆØ§Ø¹ Ø£Ø®Ø±Ù‰ Ù…Ù† Ø§Ù„ØªØµÙØ­ Ø­Ø§Ù„ÙŠØ§Ù‹
            
            print(f"ğŸ“„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© {page_num}: {url}")
            
            items = self.scrape_page(url)
            
            if not items:
                print(f"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ø§ØµØ± ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page_num}")
                break
            
            all_items.extend(items)
            print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(items)} Ø¹Ù†ØµØ± Ù…Ù† Ø§Ù„ØµÙØ­Ø© {page_num}")
            
            page_num += 1
            
            # ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª
            import time
            time.sleep(2)
        
        return all_items

# Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹ Ù…ÙˆØ§Ù‚Ø¹ Ù…Ø®ØªÙ„ÙØ©:

# 1. Ù…ÙˆÙ‚Ø¹ Ø£Ø®Ø¨Ø§Ø±
news_config = {
    'base_url': 'https://example-news.com',
    'selectors': {
        'container': '.article',
        'title': '.article-title',
        'description': '.article-summary',
        'link': 'a',
        'date': '.publish-date'
    },
    'pagination': {
        'type': 'page_number',
        'pattern': '/page/{}'
    }
}

# 2. Ù…ÙˆÙ‚Ø¹ ØªØ¬Ø§Ø±Ø© Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©
ecommerce_config = {
    'base_url': 'https://example-shop.com/products',
    'selectors': {
        'container': '.product',
        'title': '.product-name',
        'description': '.product-description',
        'price': '.price',
        'image': '.product-image img',
        'link': 'a'
    },
    'pagination': {
        'type': 'page_number',
        'pattern': '?page={}'
    }
}

# 3. Ù…ÙˆÙ‚Ø¹ ÙˆØ¸Ø§Ø¦Ù
jobs_config = {
    'base_url': 'https://example-jobs.com',
    'selectors': {
        'container': '.job-listing',
        'title': '.job-title',
        'description': '.job-description',
        'company': '.company-name',
        'location': '.job-location',
        'date': '.posted-date'
    },
    'pagination': {
        'type': 'page_number',
        'pattern': '?page={}'
    }
}

def demo_usage():
    """Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
    
    # Ø§Ø®ØªØ± Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…ÙˆÙ‚Ø¹
    config = news_config  # Ø£Ùˆ ecommerce_config Ø£Ùˆ jobs_config
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
    scraper = UniversalScraper(config)
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    items = scraper.scrape_all_pages(max_pages=5)
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    result = {
        'metadata': {
            'extraction_date': datetime.now().isoformat(),
            'total_items': len(items),
            'source_url': config['base_url']
        },
        'items': items
    }
    
    with open('universal_results.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(items)} Ø¹Ù†ØµØ± ÙˆØ­ÙØ¸Ù‡Ø§ ÙÙŠ universal_results.json")

if __name__ == "__main__":
    print("ğŸŒ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¹Ø§Ù… - ÙŠØ¹Ù…Ù„ Ù…Ø¹ Ø£ÙŠ Ù…ÙˆÙ‚Ø¹")
    print("ğŸ’¡ Ø¹Ø¯Ù„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ù„ÙŠÙ†Ø§Ø³Ø¨ Ù…ÙˆÙ‚Ø¹Ùƒ")
    # demo_usage()  # Ø£Ù„Øº Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„