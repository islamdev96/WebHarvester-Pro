"""Quick scraper to extract data from a single page."""

import requests
from bs4 import BeautifulSoup
import json
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.utils.logger import Logger
from src.scraper.data_extractor import DataExtractor
from src.scraper.json_converter import JSONConverter

def quick_scrape():
    """Quickly scrape data from the first page."""
    url = "http://www.expoegypt.gov.eg/exporters"
    
    # Fetch the page
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        print("ğŸš€ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰...")
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # Initialize components
        logger = Logger("QuickScraper", "logs/quick_scraper.log", "INFO")
        extractor = DataExtractor(logger)
        converter = JSONConverter(logger)
        
        # Extract companies
        companies = extractor.extract_companies_from_page(response.text, url)
        
        print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(companies)} Ø´Ø±ÙƒØ© Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰")
        
        if companies:
            # Clean and process data
            cleaned_companies = extractor.normalize_and_clean_data(companies)
            unique_companies = converter.remove_duplicates(cleaned_companies)
            
            # Convert to JSON
            session_info = {
                'start_time': '2025-07-22T03:05:00',
                'end_time': '2025-07-22T03:05:30',
                'total_pages_scraped': 1,
                'total_companies_extracted': len(unique_companies),
                'errors_encountered': 0
            }
            
            json_data = converter.convert_to_json(unique_companies, session_info)
            
            # Save to file
            output_file = "output/sample_exporters_data.json"
            success = converter.save_json_file(json_data, output_file)
            
            if success:
                print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ: {output_file}")
                
                # Show sample data
                print("\nğŸ“Š Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©:")
                for i, company in enumerate(unique_companies[:3]):
                    name = company.get('company_name_arabic') or company.get('company_name', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
                    phone = company.get('contact_info', {}).get('phone', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                    email = company.get('contact_info', {}).get('email', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                    sector = company.get('business_info', {}).get('categories', ['ØºÙŠØ± Ù…Ø­Ø¯Ø¯'])
                    
                    print(f"\n{i+1}. {name}")
                    print(f"   ğŸ“ Ø§Ù„Ù‡Ø§ØªÙ: {phone}")
                    print(f"   ğŸ“§ Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„: {email}")
                    print(f"   ğŸ­ Ø§Ù„Ù‚Ø·Ø§Ø¹: {', '.join(sector[:2])}")
                
                # Create summary
                summary = converter.create_summary_report(json_data)
                print(f"\nğŸ“ˆ Ù…Ù„Ø®Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
                print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø´Ø±ÙƒØ§Øª: {summary['total_companies']}")
                print(f"   Ø´Ø±ÙƒØ§Øª Ù„Ù‡Ø§ Ø¥ÙŠÙ…ÙŠÙ„: {summary['companies_with_email']}")
                print(f"   Ø´Ø±ÙƒØ§Øª Ù„Ù‡Ø§ Ù‡Ø§ØªÙ: {summary['companies_with_phone']}")
                print(f"   Ø´Ø±ÙƒØ§Øª Ù„Ù‡Ø§ Ù…ÙˆÙ‚Ø¹: {summary['companies_with_website']}")
                
                return True
            else:
                print("âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
                return False
        else:
            print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø´Ø±ÙƒØ§Øª")
            return False
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        return False

if __name__ == "__main__":
    quick_scrape()