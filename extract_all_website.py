"""Extract ALL data from the entire website."""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
import os

def extract_all_website_data():
    """Extract data from ALL pages in the website."""
    
    base_url = "http://www.expoegypt.gov.eg/exporters"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    all_companies = []
    successful_pages = 0
    failed_pages = 0
    current_page = 1
    max_consecutive_failures = 5
    consecutive_failures = 0
    
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„...")
    print("âš ï¸  Ù‡Ø°Ø§ Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹ (30-60 Ø¯Ù‚ÙŠÙ‚Ø©)")
    print("ğŸ’¡ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¨Ù€ Ctrl+C ÙÙŠ Ø£ÙŠ ÙˆÙ‚Øª ÙˆØ³ØªØ­ØªÙØ¸ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©")
    
    start_time = datetime.now()
    
    while consecutive_failures < max_consecutive_failures:
        try:
            # Build URL for current page
            if current_page == 1:
                url = base_url
            else:
                url = f"{base_url}?page={current_page}"
            
            print(f"\nğŸ“„ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© {current_page}...")
            
            # Fetch page
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            co_nodes = soup.select('.co_node')
            
            # If no companies found, might be end of pages
            if len(co_nodes) == 0:
                print(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø´Ø±ÙƒØ§Øª ÙÙŠ Ø§Ù„ØµÙØ­Ø© {current_page}")
                consecutive_failures += 1
                failed_pages += 1
                
                if consecutive_failures >= max_consecutive_failures:
                    print(f"ğŸ›‘ ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©")
                    break
                    
                current_page += 1
                continue
            
            print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(co_nodes)} Ø´Ø±ÙƒØ© ÙÙŠ Ø§Ù„ØµÙØ­Ø© {current_page}")
            
            # Reset consecutive failures counter
            consecutive_failures = 0
            
            # Extract companies from current page
            page_companies = []
            
            for i, node in enumerate(co_nodes):
                try:
                    # Extract company name
                    co_title = node.select_one('.co_title')
                    company_name = co_title.get_text().strip() if co_title else f"Ø´Ø±ÙƒØ© {i+1}"
                    
                    # Extract contact info
                    contact_info = {}
                    
                    # Phone
                    phone_elem = node.select_one('.co_phone')
                    if phone_elem:
                        phone = phone_elem.get_text().strip()
                        if phone:
                            contact_info['phone'] = phone
                    
                    # Email and website
                    co_net = node.select_one('.co_net')
                    if co_net:
                        links = co_net.find_all('a')
                        for link in links:
                            href = link.get('href', '')
                            if 'mailto:' in href:
                                contact_info['email'] = href.replace('mailto:', '')
                            elif 'www.' in href or 'http' in href:
                                contact_info['website'] = href
                    
                    # Address
                    co_address = node.select_one('.co_address')
                    if co_address:
                        address = co_address.get_text().strip()
                        if address:
                            contact_info['address'] = address
                    
                    # Business sector
                    business_info = {}
                    ind_sector = node.select_one('.ind_sector')
                    if ind_sector:
                        sector_text = ind_sector.get_text().strip()
                        if 'Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµÙ†Ø§Ø¹Ù‰:' in sector_text:
                            sector = sector_text.replace('Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµÙ†Ø§Ø¹Ù‰:', '').strip()
                            if sector:
                                business_info['sector'] = sector
                    
                    company_data = {
                        'id': f"page_{current_page}_company_{i+1}",
                        'company_name_arabic': company_name,
                        'contact_info': contact_info,
                        'business_info': business_info,
                        'source_page': current_page,
                        'source_url': url,
                        'extracted_at': datetime.now().isoformat()
                    }
                    
                    page_companies.append(company_data)
                    
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø±ÙƒØ© {i+1} Ù…Ù† Ø§Ù„ØµÙØ­Ø© {current_page}: {e}")
                    continue
            
            all_companies.extend(page_companies)
            successful_pages += 1
            
            print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(page_companies)} Ø´Ø±ÙƒØ© Ù…Ù† Ø§Ù„ØµÙØ­Ø© {current_page}")
            print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†: {len(all_companies)}")
            
            # Save progress every 10 pages
            if current_page % 10 == 0:
                save_progress(all_companies, current_page, successful_pages, start_time)
            
            # Wait between requests to be respectful
            time.sleep(2)  # 2 seconds delay
            current_page += 1
                
        except KeyboardInterrupt:
            print(f"\nâš ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
            print(f"ğŸ“Š ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(all_companies)} Ø´Ø±ÙƒØ© Ù…Ù† {successful_pages} ØµÙØ­Ø©")
            break
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© {current_page}: {e}")
            consecutive_failures += 1
            failed_pages += 1
            
            if consecutive_failures >= max_consecutive_failures:
                print(f"ğŸ›‘ ØªÙ… ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ØªØªØ§Ù„ÙŠØ©")
                break
                
            current_page += 1
            time.sleep(5)  # Wait longer after error
            continue
    
    # Final save
    end_time = datetime.now()
    duration = end_time - start_time
    
    print(f"\nğŸ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬!")
    print(f"â±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø³ØªØºØ±Ù‚: {duration}")
    print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø´Ø±ÙƒØ§Øª: {len(all_companies)}")
    print(f"ğŸ“„ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ø¨Ù†Ø¬Ø§Ø­: {successful_pages}")
    print(f"âŒ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©: {failed_pages}")
    
    # Save final results
    save_final_results(all_companies, successful_pages, failed_pages, start_time, end_time)
    
    return len(all_companies)

def save_progress(companies, current_page, successful_pages, start_time):
    """Save progress periodically."""
    print(f"ğŸ’¾ Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø¯Ù…... (Ø§Ù„ØµÙØ­Ø© {current_page})")
    
    progress_file = f"output/progress_page_{current_page}.json"
    progress_data = {
        'progress_info': {
            'current_page': current_page,
            'successful_pages': successful_pages,
            'total_companies': len(companies),
            'start_time': start_time.isoformat(),
            'last_update': datetime.now().isoformat()
        },
        'companies': companies
    }
    
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress_data, f, ensure_ascii=False, indent=2)

def save_final_results(companies, successful_pages, failed_pages, start_time, end_time):
    """Save final comprehensive results."""
    
    # Create results
    result = {
        'metadata': {
            'extraction_date': end_time.isoformat(),
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'duration_seconds': (end_time - start_time).total_seconds(),
            'total_pages_extracted': successful_pages,
            'failed_pages': failed_pages,
            'total_companies': len(companies),
            'base_url': "http://www.expoegypt.gov.eg/exporters",
            'extraction_type': 'FULL_WEBSITE'
        },
        'companies': companies
    }
    
    # Save main results file
    output_file = "output/FULL_WEBSITE_DATA.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    # Create comprehensive summary
    summary_file = "output/FULL_EXTRACTION_SUMMARY.txt"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("=== Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØ§Ù…Ù„ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹ ===\n\n")
        f.write(f"ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø¯Ø¡: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø³ØªØºØ±Ù‚: {end_time - start_time}\n")
        f.write(f"Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ø¨Ù†Ø¬Ø§Ø­: {successful_pages}\n")
        f.write(f"Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©: {failed_pages}\n")
        f.write(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {len(companies)}\n\n")
        
        # Statistics
        companies_with_phone = sum(1 for c in companies if c.get('contact_info', {}).get('phone'))
        companies_with_email = sum(1 for c in companies if c.get('contact_info', {}).get('email'))
        companies_with_website = sum(1 for c in companies if c.get('contact_info', {}).get('website'))
        
        f.write("=== Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© ===\n")
        f.write(f"Ø´Ø±ÙƒØ§Øª Ù„Ù‡Ø§ Ù‡Ø§ØªÙ: {companies_with_phone} ({companies_with_phone/len(companies)*100:.1f}%)\n")
        f.write(f"Ø´Ø±ÙƒØ§Øª Ù„Ù‡Ø§ Ø¥ÙŠÙ…ÙŠÙ„: {companies_with_email} ({companies_with_email/len(companies)*100:.1f}%)\n")
        f.write(f"Ø´Ø±ÙƒØ§Øª Ù„Ù‡Ø§ Ù…ÙˆÙ‚Ø¹: {companies_with_website} ({companies_with_website/len(companies)*100:.1f}%)\n\n")
        
        # Sector analysis
        sectors = {}
        for company in companies:
            sector = company.get('business_info', {}).get('sector', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            sectors[sector] = sectors.get(sector, 0) + 1
        
        f.write("=== ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù‚Ø·Ø§Ø¹Ø§Øª ===\n")
        for sector, count in sorted(sectors.items(), key=lambda x: x[1], reverse=True):
            f.write(f"{sector}: {count} Ø´Ø±ÙƒØ©\n")
        
        f.write(f"\n=== Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø´Ø±ÙƒØ§Øª (Ø£ÙˆÙ„ 20) ===\n")
        for i, company in enumerate(companies[:20]):
            name = company.get('company_name_arabic', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            phone = company.get('contact_info', {}).get('phone', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
            email = company.get('contact_info', {}).get('email', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
            sector = company.get('business_info', {}).get('sector', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            page = company.get('source_page', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            
            f.write(f"\n{i+1}. {name} (ØµÙØ­Ø© {page})\n")
            f.write(f"   Ø§Ù„Ù‡Ø§ØªÙ: {phone}\n")
            f.write(f"   Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„: {email}\n")
            f.write(f"   Ø§Ù„Ù‚Ø·Ø§Ø¹: {sector}\n")
    
    # Create comprehensive CSV file
    csv_file = "output/FULL_COMPANIES_DATABASE.csv"
    with open(csv_file, 'w', encoding='utf-8-sig') as f:
        f.write("Ø§Ù„Ø±Ù‚Ù…,Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ©,Ø§Ù„Ù‡Ø§ØªÙ,Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„,Ø§Ù„Ù…ÙˆÙ‚Ø¹,Ø§Ù„Ø¹Ù†ÙˆØ§Ù†,Ø§Ù„Ù‚Ø·Ø§Ø¹,Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø©,ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬\n")
        
        for i, company in enumerate(companies):
            name = company.get('company_name_arabic', '').replace(',', 'ØŒ')
            phone = company.get('contact_info', {}).get('phone', '')
            email = company.get('contact_info', {}).get('email', '')
            website = company.get('contact_info', {}).get('website', '')
            address = company.get('contact_info', {}).get('address', '').replace(',', 'ØŒ').replace('\n', ' ')
            sector = company.get('business_info', {}).get('sector', '')
            page = company.get('source_page', '')
            extracted_at = company.get('extracted_at', '')
            
            f.write(f"{i+1},{name},{phone},{email},{website},{address},{sector},{page},{extracted_at}\n")
    
    print(f"\nğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print(f"   ğŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©: {output_file}")
    print(f"   ğŸ“ Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø´Ø§Ù…Ù„: {summary_file}")
    print(f"   ğŸ“Š Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª CSV: {csv_file}")

if __name__ == "__main__":
    print("ğŸŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØ§Ù…Ù„ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ØµØ¯Ø±ÙŠÙ† Ø§Ù„Ù…ØµØ±ÙŠÙŠÙ†")
    print("=" * 50)
    
    try:
        total_companies = extract_all_website_data()
        print(f"\nâœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {total_companies} Ø´Ø±ÙƒØ© Ø¨Ù†Ø¬Ø§Ø­ Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„!")
        print("ğŸ” ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø¬Ù„Ø¯ output Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
        print("ğŸ’¾ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ø­ØªÙ‰ Ø§Ù„Ø¢Ù† Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ output")
        
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬: {e}")
        print("ğŸ’¾ ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø¬Ù„Ø¯ output Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©")