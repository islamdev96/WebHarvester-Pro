"""
Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ Ù…Ø®ØªÙ„Ø· - Ø§Ù„Ø£ÙØ¶Ù„ Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†
Framework Ù…Ø´ØªØ±Ùƒ + ØªØ®ØµÙŠØµ Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹
"""

from abc import ABC, abstractmethod
import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional

class BaseScraper(ABC):
    """Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© Ù„ÙƒÙ„ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.base_url = config['base_url']
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.delay = config.get('delay', 2)
        self.timeout = config.get('timeout', 30)
        self.max_retries = config.get('max_retries', 3)
    
    def fetch_page(self, url: str) -> Optional[str]:
        """Ø¬Ù„Ø¨ ØµÙØ­Ø© Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© - Ù…Ø´ØªØ±Ùƒ Ù„ÙƒÙ„ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹"""
        for attempt in range(self.max_retries):
            try:
                response = requests.get(url, headers=self.headers, timeout=self.timeout)
                response.raise_for_status()
                return response.text
            except Exception as e:
                print(f"Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1} ÙØ´Ù„Øª: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # ØªØ£Ø®ÙŠØ± Ù…ØªØ²Ø§ÙŠØ¯
        return None
    
    def save_results(self, data: List[Dict], filename: str):
        """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ - Ù…Ø´ØªØ±Ùƒ Ù„ÙƒÙ„ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹"""
        result = {
            'metadata': {
                'extraction_date': datetime.now().isoformat(),
                'total_items': len(data),
                'source': self.base_url,
                'scraper_type': self.__class__.__name__
            },
            'data': data
        }
        
        with open(f"output/{filename}.json", 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ØªÙ… Ø­ÙØ¸ {len(data)} Ø¹Ù†ØµØ± ÙÙŠ {filename}.json")
    
    def respect_rate_limits(self):
        """Ø§Ø­ØªØ±Ø§Ù… Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„ - Ù…Ø´ØªØ±Ùƒ"""
        time.sleep(self.delay)
    
    # Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø¬Ø±Ø¯Ø© - ÙŠØ¬Ø¨ ØªÙ†ÙÙŠØ°Ù‡Ø§ ÙÙŠ ÙƒÙ„ Ù…ÙˆÙ‚Ø¹
    @abstractmethod
    def extract_items_from_page(self, html: str, page_url: str) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ù…Ù† ØµÙØ­Ø© - Ù…Ø®ØµØµ Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹"""
        pass
    
    @abstractmethod
    def get_next_page_url(self, current_url: str, page_num: int) -> Optional[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© - Ù…Ø®ØµØµ Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹"""
        pass
    
    @abstractmethod
    def is_valid_item(self, item: Dict[str, Any]) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¹Ù†ØµØ± - Ù…Ø®ØµØµ Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹"""
        pass
    
    def scrape_all_pages(self, max_pages: Optional[int] = None) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø§Ù„ØµÙØ­Ø§Øª - Ù…Ø´ØªØ±Ùƒ Ù…Ø¹ ØªØ®ØµÙŠØµ"""
        all_items = []
        page_num = 1
        current_url = self.base_url
        
        while True:
            if max_pages and page_num > max_pages:
                break
            
            print(f"ğŸ“„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© {page_num}: {current_url}")
            
            html = self.fetch_page(current_url)
            if not html:
                print(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© {page_num}")
                break
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø®ØµØµ Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹
            items = self.extract_items_from_page(html, current_url)
            
            if not items:
                print(f"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ø§ØµØ± ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page_num}")
                break
            
            # ØªØµÙÙŠØ© Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØµØ­ÙŠØ­Ø©
            valid_items = [item for item in items if self.is_valid_item(item)]
            all_items.extend(valid_items)
            
            print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(valid_items)} Ø¹Ù†ØµØ± ØµØ­ÙŠØ­ Ù…Ù† Ø§Ù„ØµÙØ­Ø© {page_num}")
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
            next_url = self.get_next_page_url(current_url, page_num + 1)
            if not next_url:
                break
            
            current_url = next_url
            page_num += 1
            self.respect_rate_limits()
        
        return all_items

# ØªØ·Ø¨ÙŠÙ‚ Ù…Ø®ØµØµ Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ØµØ¯Ø±ÙŠÙ† Ø§Ù„Ù…ØµØ±ÙŠÙŠÙ†
class EgyptExportersScraper(BaseScraper):
    """Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ø®ØµØµ Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ØµØ¯Ø±ÙŠÙ† Ø§Ù„Ù…ØµØ±ÙŠÙŠÙ†"""
    
    def extract_items_from_page(self, html: str, page_url: str) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø±ÙƒØ§Øª Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
        soup = BeautifulSoup(html, 'lxml')
        companies = []
        
        co_nodes = soup.select('.co_node')
        
        for i, node in enumerate(co_nodes):
            try:
                # Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ©
                co_title = node.select_one('.co_title')
                company_name = co_title.get_text().strip() if co_title else ""
                
                # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„
                contact_info = {}
                
                # Ø§Ù„Ù‡Ø§ØªÙ
                phone_elem = node.select_one('.co_phone')
                if phone_elem:
                    phone = phone_elem.get_text().strip()
                    if phone:
                        contact_info['phone'] = phone
                
                # Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„ ÙˆØ§Ù„Ù…ÙˆÙ‚Ø¹
                co_net = node.select_one('.co_net')
                if co_net:
                    links = co_net.find_all('a')
                    for link in links:
                        href = link.get('href', '')
                        if 'mailto:' in href:
                            contact_info['email'] = href.replace('mailto:', '')
                        elif 'www.' in href or 'http' in href:
                            contact_info['website'] = href
                
                # Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                co_address = node.select_one('.co_address')
                if co_address:
                    contact_info['address'] = co_address.get_text().strip()
                
                # Ø§Ù„Ù‚Ø·Ø§Ø¹
                sector = ""
                ind_sector = node.select_one('.ind_sector')
                if ind_sector:
                    sector_text = ind_sector.get_text().strip()
                    if 'Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµÙ†Ø§Ø¹Ù‰:' in sector_text:
                        sector = sector_text.replace('Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµÙ†Ø§Ø¹Ù‰:', '').strip()
                
                company = {
                    'id': f"company_{i+1}",
                    'name': company_name,
                    'contact_info': contact_info,
                    'sector': sector,
                    'source_page': page_url,
                    'extracted_at': datetime.now().isoformat()
                }
                
                companies.append(company)
                
            except Exception as e:
                print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø±ÙƒØ© {i+1}: {e}")
                continue
        
        return companies
    
    def get_next_page_url(self, current_url: str, page_num: int) -> Optional[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©"""
        if page_num == 1:
            return self.base_url
        else:
            return f"{self.base_url}?page={page_num}"
    
    def is_valid_item(self, item: Dict[str, Any]) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø´Ø±ÙƒØ©"""
        return bool(item.get('name') and item['name'].strip())

# ØªØ·Ø¨ÙŠÙ‚ Ù…Ø®ØµØµ Ù„Ù…ÙˆÙ‚Ø¹ Ø£Ø®Ø¨Ø§Ø±
class NewsScraper(BaseScraper):
    """Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ø®ØµØµ Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±"""
    
    def extract_items_from_page(self, html: str, page_url: str) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
        soup = BeautifulSoup(html, 'lxml')
        articles = []
        
        # Ù…Ø­Ø¯Ø¯Ø§Øª Ù…Ø±Ù†Ø© Ù„Ù„Ø£Ø®Ø¨Ø§Ø±
        selectors = self.config.get('selectors', {
            'container': '.article, .news-item, .post',
            'title': 'h1, h2, h3, .title, .headline',
            'summary': '.summary, .excerpt, .description',
            'date': '.date, .published, .timestamp',
            'link': 'a'
        })
        
        containers = soup.select(selectors['container'])
        
        for i, container in enumerate(containers):
            try:
                title_elem = container.select_one(selectors['title'])
                title = title_elem.get_text().strip() if title_elem else ""
                
                summary_elem = container.select_one(selectors['summary'])
                summary = summary_elem.get_text().strip() if summary_elem else ""
                
                date_elem = container.select_one(selectors['date'])
                date = date_elem.get_text().strip() if date_elem else ""
                
                link_elem = container.select_one(selectors['link'])
                link = link_elem.get('href', '') if link_elem else ""
                
                article = {
                    'id': f"article_{i+1}",
                    'title': title,
                    'summary': summary,
                    'date': date,
                    'link': link,
                    'source_page': page_url,
                    'extracted_at': datetime.now().isoformat()
                }
                
                articles.append(article)
                
            except Exception as e:
                print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù‚Ø§Ù„ {i+1}: {e}")
                continue
        
        return articles
    
    def get_next_page_url(self, current_url: str, page_num: int) -> Optional[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©"""
        pattern = self.config.get('pagination_pattern', '/page/{}')
        return self.base_url + pattern.format(page_num)
    
    def is_valid_item(self, item: Dict[str, Any]) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ù‚Ø§Ù„"""
        return bool(item.get('title') and len(item['title']) > 10)

# ØªØ·Ø¨ÙŠÙ‚ Ù…Ø®ØµØµ Ù„Ù„Ù…ØªØ§Ø¬Ø± Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©
class EcommerceScraper(BaseScraper):
    """Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ø®ØµØµ Ù„Ù„Ù…ØªØ§Ø¬Ø± Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©"""
    
    def extract_items_from_page(self, html: str, page_url: str) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
        soup = BeautifulSoup(html, 'lxml')
        products = []
        
        selectors = self.config.get('selectors', {
            'container': '.product, .item, .product-card',
            'title': '.product-name, .title, h3',
            'price': '.price, .cost, .amount',
            'image': 'img',
            'rating': '.rating, .stars',
            'link': 'a'
        })
        
        containers = soup.select(selectors['container'])
        
        for i, container in enumerate(containers):
            try:
                title_elem = container.select_one(selectors['title'])
                title = title_elem.get_text().strip() if title_elem else ""
                
                price_elem = container.select_one(selectors['price'])
                price = price_elem.get_text().strip() if price_elem else ""
                
                image_elem = container.select_one(selectors['image'])
                image = image_elem.get('src', '') if image_elem else ""
                
                rating_elem = container.select_one(selectors['rating'])
                rating = rating_elem.get_text().strip() if rating_elem else ""
                
                link_elem = container.select_one(selectors['link'])
                link = link_elem.get('href', '') if link_elem else ""
                
                product = {
                    'id': f"product_{i+1}",
                    'title': title,
                    'price': price,
                    'image': image,
                    'rating': rating,
                    'link': link,
                    'source_page': page_url,
                    'extracted_at': datetime.now().isoformat()
                }
                
                products.append(product)
                
            except Exception as e:
                print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù†ØªØ¬ {i+1}: {e}")
                continue
        
        return products
    
    def get_next_page_url(self, current_url: str, page_num: int) -> Optional[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©"""
        pattern = self.config.get('pagination_pattern', '?page={}')
        return self.base_url + pattern.format(page_num)
    
    def is_valid_item(self, item: Dict[str, Any]) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ù†ØªØ¬"""
        return bool(item.get('title') and item.get('price'))

# Ù…ØµÙ†Ø¹ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø§Øª
class ScraperFactory:
    """Ù…ØµÙ†Ø¹ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹"""
    
    @staticmethod
    def create_scraper(website_type: str, config: Dict[str, Any]) -> BaseScraper:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨"""
        scrapers = {
            'egypt_exporters': EgyptExportersScraper,
            'news': NewsScraper,
            'ecommerce': EcommerceScraper
        }
        
        scraper_class = scrapers.get(website_type)
        if not scraper_class:
            raise ValueError(f"Ù†ÙˆØ¹ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {website_type}")
        
        return scraper_class(config)

# Ø£Ù…Ø«Ù„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
def example_usage():
    """Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
    
    # 1. Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ØµØ¯Ø±ÙŠÙ† Ø§Ù„Ù…ØµØ±ÙŠÙŠÙ†
    egypt_config = {
        'base_url': 'http://www.expoegypt.gov.eg/exporters',
        'delay': 2,
        'timeout': 30
    }
    
    egypt_scraper = ScraperFactory.create_scraper('egypt_exporters', egypt_config)
    companies = egypt_scraper.scrape_all_pages(max_pages=5)
    egypt_scraper.save_results(companies, 'egypt_companies')
    
    # 2. Ù…ÙˆÙ‚Ø¹ Ø£Ø®Ø¨Ø§Ø±
    news_config = {
        'base_url': 'https://example-news.com',
        'pagination_pattern': '/page/{}',
        'selectors': {
            'container': '.article',
            'title': '.article-title',
            'summary': '.article-summary',
            'date': '.publish-date'
        }
    }
    
    news_scraper = ScraperFactory.create_scraper('news', news_config)
    articles = news_scraper.scrape_all_pages(max_pages=3)
    news_scraper.save_results(articles, 'news_articles')
    
    # 3. Ù…ØªØ¬Ø± Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ
    shop_config = {
        'base_url': 'https://example-shop.com/products',
        'pagination_pattern': '?page={}',
        'selectors': {
            'container': '.product',
            'title': '.product-name',
            'price': '.price',
            'image': '.product-image img'
        }
    }
    
    shop_scraper = ScraperFactory.create_scraper('ecommerce', shop_config)
    products = shop_scraper.scrape_all_pages(max_pages=2)
    shop_scraper.save_results(products, 'shop_products')

if __name__ == "__main__":
    print("ğŸ—ï¸ Ø¥Ø·Ø§Ø± Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ù…Ø®ØªÙ„Ø· - Ø§Ù„Ø£ÙØ¶Ù„ Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†")
    print("=" * 50)
    # example_usage()  # Ø£Ù„Øº Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ù„Ù„ØªØ´ØºÙŠÙ„