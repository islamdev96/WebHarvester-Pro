"""Extract data from multiple pages and save results."""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
import os

def extract_from_multiple_pages(max_pages=5):
    """Extract data from multiple pages."""
    
    base_url = "http://www.expoegypt.gov.eg/exporters"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    all_companies = []
    successful_pages = 0
    
    print(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† {max_pages} ØµÙØ­Ø§Øª...")
    
    for page_num in range(1, max_pages + 1):
        try:
            # Build URL for current page
            if page_num == 1:
                url = base_url
            else:
                url = f"{base_url}?page={page_num}"
            
            print(f"\nğŸ“„ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© {page_num}...")
            print(f"ğŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: {url}")
            
            # Fetch page
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            co_nodes = soup.select('.co_node')
            
            print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(co_nodes)} Ø´Ø±ÙƒØ© ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page_num}")
            
            # Extract companies from current page
            page_companies = []
            
            for i, node in enumerate(co_nodes):
                try:
                    # Extract company name
                    co_title = node.select_one('.co_title')
                    company_name = co_title.get_text().strip() if co_title else f"Ø´Ø±ÙƒØ© {i+1}"
                    
                    # Extract contact info
                    contact_info = {}
                    
                    # Phone
                    phone_elem = node.select_one('.co_phone')
                    if phone_elem:
                        phone = phone_elem.get_text().strip()
                        if phone:
                            contact_info['phone'] = phone
                    
                    # Email and website
                    co_net = node.select_one('.co_net')
                    if co_net:
                        links = co_net.find_all('a')
                        for link in links:
                            href = link.get('href', '')
                            if 'mailto:' in href:
                                contact_info['email'] = href.replace('mailto:', '')
                            elif 'www.' in href or 'http' in href:
                                contact_info['website'] = href
                    
                    # Address
                    co_address = node.select_one('.co_address')
                    if co_address:
                        address = co_address.get_text().strip()
                        if address:
                            contact_info['address'] = address
                    
                    # Business sector
                    business_info = {}
                    ind_sector = node.select_one('.ind_sector')
                    if ind_sector:
                        sector_text = ind_sector.get_text().strip()
                        if 'Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµÙ†Ø§Ø¹Ù‰:' in sector_text:
                            sector = sector_text.replace('Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµÙ†Ø§Ø¹Ù‰:', '').strip()
                            if sector:
                                business_info['sector'] = sector
                    
                    company_data = {
                        'id': f"page_{page_num}_company_{i+1}",
                        'company_name_arabic': company_name,
                        'contact_info': contact_info,
                        'business_info': business_info,
                        'source_page': page_num,
                        'source_url': url,
                        'extracted_at': datetime.now().isoformat()
                    }
                    
                    page_companies.append(company_data)
                    
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø±ÙƒØ© {i+1} Ù…Ù† Ø§Ù„ØµÙØ­Ø© {page_num}: {e}")
                    continue
            
            all_companies.extend(page_companies)
            successful_pages += 1
            
            print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(page_companies)} Ø´Ø±ÙƒØ© Ù…Ù† Ø§Ù„ØµÙØ­Ø© {page_num}")
            
            # Wait between requests to be respectful
            if page_num < max_pages:
                print("â³ Ø§Ù†ØªØ¸Ø§Ø± 3 Ø«ÙˆØ§Ù†...")
                time.sleep(3)
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© {page_num}: {e}")
            continue
    
    # Create results
    result = {
        'metadata': {
            'extraction_date': datetime.now().isoformat(),
            'total_pages_extracted': successful_pages,
            'total_companies': len(all_companies),
            'pages_requested': max_pages,
            'base_url': base_url
        },
        'companies': all_companies
    }
    
    # Save main results file
    output_file = "output/multiple_pages_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    # Create a summary file
    summary_file = "output/extraction_summary.txt"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("=== Ù…Ù„Ø®Øµ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===\n\n")
        f.write(f"ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: {max_pages}\n")
        f.write(f"Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ø¨Ù†Ø¬Ø§Ø­: {successful_pages}\n")
        f.write(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {len(all_companies)}\n\n")
        
        # Statistics
        companies_with_phone = sum(1 for c in all_companies if c.get('contact_info', {}).get('phone'))
        companies_with_email = sum(1 for c in all_companies if c.get('contact_info', {}).get('email'))
        companies_with_website = sum(1 for c in all_companies if c.get('contact_info', {}).get('website'))
        
        f.write("=== Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ===\n")
        f.write(f"Ø´Ø±ÙƒØ§Øª Ù„Ù‡Ø§ Ù‡Ø§ØªÙ: {companies_with_phone}\n")
        f.write(f"Ø´Ø±ÙƒØ§Øª Ù„Ù‡Ø§ Ø¥ÙŠÙ…ÙŠÙ„: {companies_with_email}\n")
        f.write(f"Ø´Ø±ÙƒØ§Øª Ù„Ù‡Ø§ Ù…ÙˆÙ‚Ø¹: {companies_with_website}\n\n")
        
        # Sample companies
        f.write("=== Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø´Ø±ÙƒØ§Øª ===\n")
        for i, company in enumerate(all_companies[:10]):
            name = company.get('company_name_arabic', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            phone = company.get('contact_info', {}).get('phone', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
            email = company.get('contact_info', {}).get('email', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
            sector = company.get('business_info', {}).get('sector', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            page = company.get('source_page', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            
            f.write(f"\n{i+1}. {name} (ØµÙØ­Ø© {page})\n")
            f.write(f"   Ø§Ù„Ù‡Ø§ØªÙ: {phone}\n")
            f.write(f"   Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„: {email}\n")
            f.write(f"   Ø§Ù„Ù‚Ø·Ø§Ø¹: {sector}\n")
    
    # Create Excel-like CSV file
    csv_file = "output/companies_data.csv"
    with open(csv_file, 'w', encoding='utf-8-sig') as f:
        f.write("Ø§Ù„Ø±Ù‚Ù…,Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ©,Ø§Ù„Ù‡Ø§ØªÙ,Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„,Ø§Ù„Ù…ÙˆÙ‚Ø¹,Ø§Ù„Ø¹Ù†ÙˆØ§Ù†,Ø§Ù„Ù‚Ø·Ø§Ø¹,Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø©\n")
        
        for i, company in enumerate(all_companies):
            name = company.get('company_name_arabic', '').replace(',', 'ØŒ')
            phone = company.get('contact_info', {}).get('phone', '')
            email = company.get('contact_info', {}).get('email', '')
            website = company.get('contact_info', {}).get('website', '')
            address = company.get('contact_info', {}).get('address', '').replace(',', 'ØŒ').replace('\n', ' ')
            sector = company.get('business_info', {}).get('sector', '')
            page = company.get('source_page', '')
            
            f.write(f"{i+1},{name},{phone},{email},{website},{address},{sector},{page}\n")
    
    print(f"\nğŸ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬!")
    print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø´Ø±ÙƒØ§Øª: {len(all_companies)}")
    print(f"ğŸ“„ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {successful_pages} Ù…Ù† {max_pages}")
    
    print(f"\nğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©:")
    print(f"   ğŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©: {output_file}")
    print(f"   ğŸ“ Ø§Ù„Ù…Ù„Ø®Øµ: {summary_file}")
    print(f"   ğŸ“Š Ù…Ù„Ù CSV: {csv_file}")
    
    # Show sample results
    print(f"\nğŸ¢ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©:")
    for i, company in enumerate(all_companies[:5]):
        name = company.get('company_name_arabic', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
        phone = company.get('contact_info', {}).get('phone', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
        email = company.get('contact_info', {}).get('email', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
        sector = company.get('business_info', {}).get('sector', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
        page = company.get('source_page', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
        
        print(f"\n{i+1}. {name} (ØµÙØ­Ø© {page})")
        print(f"   ğŸ“ Ø§Ù„Ù‡Ø§ØªÙ: {phone}")
        print(f"   ğŸ“§ Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„: {email}")
        print(f"   ğŸ­ Ø§Ù„Ù‚Ø·Ø§Ø¹: {sector}")
    
    return len(all_companies)

if __name__ == "__main__":
    # Extract from 10 pages (you can change this number)
    pages_to_extract = 10
    print(f"Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† {pages_to_extract} ØµÙØ­Ø§Øª")
    print("ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ± Ù‡Ø°Ø§ Ø§Ù„Ø±Ù‚Ù… ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø£ÙƒØ«Ø± Ø£Ùˆ Ø£Ù‚Ù„")
    
    total_companies = extract_from_multiple_pages(pages_to_extract)
    
    print(f"\nâœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {total_companies} Ø´Ø±ÙƒØ© Ø¨Ù†Ø¬Ø§Ø­!")
    print("ğŸ” ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† ÙØªØ­ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ output Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬")